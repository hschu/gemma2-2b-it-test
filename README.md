# 로컬 PC에서 Gemma2 2B 모델 테스트

## 1. 들어가며
이 저장소는 **트랜스포머부터 이해하는 생성 인공지능 100제**에 수록된 소규모 거대 언어 모델(smaller Large Language Model, 이하 sLLM) 테스트를 다루고 있습니다. 모델은 구글에서 2024년 8월 공개한 오픈소스 sLLM인 `gemma2`이며, PC에서 CPU만을 사용하여 모델을 시험해 보는 과정을 다루고 있습니다.

sLLM이라는 용어는 2023년 초 Meta가 개발한 오픈소스 거대 언어 모델(Large Language Model, 이하 LLM)인 `LLaMA`로 부터 출발합니다. `LLaMA`는 계산량의 관점에서는 여전히 GPT-3에 육박하기 때문에, LLaMA의 논문에서도 **거대 언어 모델**이라고 명시하고 있습니다. 그러나 인공지능 커뮤니티에서는 `LLaMA`가 기존의 LLM보다는 훨씬 작은 규모의 모델이기 때문에 편의상 sLLM이라는 용어를 사용한 것으로 추정됩니다. 또한 마이크로소프트가 개발한 sLLM인 `phi`는 소형 언어 모델(small Language Model, sLM)이라는 표현을 사용하고 있어 약간 혼란스럽기도 합니다. 저서에서는 2023년 이후에 나온 70B 이하의 경량화 모델을 sLLM으로 정의했습니다. 

sLM이든 sLLM이든 대부분 오픈소스이며 작은 규모의 모델이기 때문에, 보급형 GPU가 탑재된 데스크탑만 가지고 있다면 누구나 쉽게 사용해 볼 수 있습니다. 그러나 오픈소스의 특성상 진입장벽이 있을 수 밖에 없기 때문에, 언어 모델을 개발하고 있는 주요 빅테크 기업은 **플랫폼**의 형태로 손쉽게 오픈소스 모델을 개발하고 활용할 수 있는 환경을 제공합니다. 이러한 플랫폼에서는 특정한 데이터로 재학습시킨 특화 모델을 손쉽게 공유할 수 있고, 클라우드의 컴퓨팅 인스턴스를 필요할 때마다 사용하는 방식으로 비용을 절감할 수 있기 때문에 인공지능 개발 생태계가 플랫폼으로 변화하고 있습니다.

하지만 플랫폼을 사용한다는 것은 대부분 인터넷 연결을 전제로 하고 있고, 내 컴퓨터가 아닌 원격 컴퓨팅 자원을 활용한다는 점이 상황에 따라서 단점일 수 있습니다. 이에 이 저장소에서는 로컬 PC에서 sLLM을 구동해보는 실습을 진행해보고자 합니다. 특히 환경을 설정하고 모델을 다운로드 받는 과정에 일정한 절차가 필요합니다. 이러한 과정을 모두 영상에 담아놨으니 하나씩 따라하신다면 무리 없이 실행을 해보실 수 있습니다. 

## 2. 환경 설정
컴퓨터 프로그래밍을 하다보면 환경 설정(configuration)을 하는데 소요되는 시간이 상당합니다. 특히 파이썬과 같은 언어에서는 패키지의 버전에 따라 실행이 되고 안되는 경우가 있습니다. 대부분은 오류 메시지로 인해 작성한 소스코드가 실행되지 않기 때문에, 구글링하여 해법을 찾는 것이 일반적입니다. 오류 메시지는 컴퓨터에 설치되어 있는 각종 프로그램이나 라이브러리에 따라서, 혹은 운영체제에 따라서도 달라질 수 있습니다. 따라서 모든 오류를 처리하고 프로그램을 실행시키기란 쉬운 일은 아닙니다. 

이러한 문제로 인해 컨테이너와 같은 기술을 사용할 수 있지만, 그것은 그것대로 사용하는데 있어 진입장벽이 있기 때문에 이 저장소에서는 전통적인 방식인 IDE를 활용해 컴퓨터에서 프로그램을 실행해 보고자 합니다. 이 저장소는 다음의 환경에서 실행되었습니다.
 * 운영체제 : `Windows 11`
 * 통합개발환경 : `PyCharm Community Edition 2024.2.0.1`
 * 프로그래밍 언어 : `Python 3.12.3`

### 2.1 PyCharm IDE 설치
PyCharm Community Edition은 무료로 사용할수 있는 대표적인 파이썬 프로그래밍 도구로 원활한 코딩을 지원해주는 많은 기능을 담고 있습니다. 이 저장소에서는 2024.2.0.1 버전을 사용하며, 최신 버전을 사용해도 무방할 것으로 판단됩니다만, 자동으로 설치되는 파이썬 버전에 따라서 프로그램이 실행되지 않을수도 있습니다. <U>[링크](https://www.jetbrains.com/ko-kr/pycharm/download/other.html)</U>를 따라가시면 나오는 페이지에서 `2024.2.0.1 - Windows (exe)` 를 클릭하고 다운받은 뒤 설치를 진행하면 됩니다. 설치가 완료되면 영상에 따라 진행하면 자동으로 `Python 3.12` 버전이 설치됩니다. 설치하는 시기에 따라서 버전이 변경될 수도 있습니다. 이 경우에는 별도로 파이썬을 설치하고 Interpreter를 수동으로 선택하면 됩니다. 

PyCharm과 같이 PC에 설치하는 IDE이외에 각종 코딩 플랫폼에서 제공하는 웹기반 IDE도 있습니다만, 대부분 인터넷 연결을 전제로 하고 있다는 점을 고려해야 합니다.

### 2.2 Git을 통한 소스코드 다운로드
PyCharm을 실행하여 새 프로젝트를 만들 때, github 저장소에 있는 소스코드를 그대로 복사하여 프로젝트로 만들 수 있습니다. 이 때 Windows에서 실행할 수 있는 GIT 프로그램을 설치해야하며, 영상을 참고하여 진행하면 됩니다. 

### 2.3 파이썬 패키지 설치
파이썬이라는 프로그래밍 언어는 컴퓨터와 대화하는 형식으로 프로그래밍 할 수 있는 인터프리터(Interpreter) 입니다. 파이썬은 무료로 사용할 수 있다는 점과 조금 더 직관적으로 컴퓨터 프로그래밍을 할 수 있다는 것이 장점입니다. 특히 무료라는 이점 때문에 다양한 개발자들이 유용한 패키지(특정 기능을 구현하기 위한 프로그램의 집합체)를 개발하고 또 다시 무료로 공개하여 선순환 생태계를 만들었습니다. 사용자의 입장에서는 다양한 기능을 구현할 수 있다는 장점이 있으나, 최적화 수준은 패키지 별로 제각각이기 때문에 많은 작업량을 요구하는 경우 문제가 발생할 수 있습니다. 

파이썬은 특히 딥러닝과 궁합이 좋았습니다. 파이썬의 단점은 낮은 계산 효율이었는데, 이것을 거대 테크 기업들이 직접 패키지를 개발하여 효율을 확보했다는 점입니다. 가장 밑바탕이 되는 GPU(Graphical Processing Unit)의 병렬처리 알고리즘부터, 이 병렬처리의 성능적인 손실을 최소화하기 위한 엔지니어링이 추가되어 사용자는 큰 어려움 없이 간소화된 프로그래밍만으로도 복잡한 딥러닝을 구현할 수 있습니다. 이것은 소위 딥러닝 프레임워크로 불리우며 대표적으로는 구글의 `Tensorflow`와 `JAX`, 메타의 `pytorch`가 있습니다. 

`huggingface`의 언어 모델 라이브러리인 `transformers`는 다양한 sLLM을 효율적으로 불러오고 활용할 수 있는 기능들을 지원합니다. 모델 별로 상이한 어휘 사전, 토큰화 알고리즘, 모델 구조, 모델에서 제공하는 기능을 일관된 인터페이스로 제공하여 사용자들은 손쉽게 언어 모델을 다룰 수 있습니다. 

이러한 패키지들은 모두 무료로 공개가 되어있다는 것이 장점이지만, 전술한 환경 설정의 측면에서는 매우 유감스러운 상황이 발생합니다. 패키지의 버전을 모두 최신으로 설치하면 열에 아홉은 오류가 나오게 마련입니다. 적당한 버전을 찾아서 설치하는 것이 중요한데, 여기에는 뾰족한 방법이 없으므로 일단 해보고 수정해가는 것이 일반적입니다. 다음은 `gemma2`를 재학습하기 위해 별도로 설치한 라이브러리 버전입니다. 이 버전은 `requirements.txt`에 담겨있는 것으로 PyCharm에서 프로젝트를 생성하면 클릭 몇 번으로 바로 설치가 가능합니다.

<div align="center">

| 패키지          | 버전     | 기능                     |
|--------------|--------|------------------------|
| `torch`        | 2.4.1  | 딥러닝 프레임워크인 pytorch     |
| `transformers` | 4.44.2 | sLLM 모델을 불러오고 사용하는데 활용 |

</div>
 

## 3. 로컬 PC에서 gemma2 2B 모델 테스트

### 3.1 개요
`gemma2`는 2024년 6월 구글이 개발한 sLLM으로 모수의 수가 2B, 9B, 27B로 구성되어 있습니다. 이 저장소에서는 가장 작은 2B모델을 사용하며, ChatGPT와 같이 유해성과 편향성 등이 완화된 IT(Instruction-Tuned) 모델을 사용합니다. 이 모델의 세부사항은 `huggingface` [저장소](https://huggingface.co/google/gemma-2-2b-it)에서 확인할 수 있습니다. 

### 3.2 실행 환경
실행 환경은 노트북이나 사무용 PC에서 CPU만을 사용합니다. 2B 모델이지만 산술적으로 32-bit 단정밀도(single precision)로 모수를 저장할 경우, 필요한 여유 메모리(RAM) 공간이 8GB 필요합니다. 16-bit 반정밀도의 경우에는 절반인 4GB입니다. 메모리가 부족한 경우에는 매우 많은 시간이 필요할 수 있기 때문에 주의가 필요합니다.

### 3.3 gemma2 다운로드
`gemma2`를 다운로드 받기 위해서는 **구글 계정**과 **허깅페이스 계정**이 필요합니다. **구글 계정**은 `gemma2` 모델에 대한 사용 동의를 하는데 필요하고, **허깅페이스 계정**은 `gemma2` 사용 동의가 완료되었다는 것을 인증받기 위한 Token을 생성하는데 필요합니다. 일련의 과정은 영상에 담겨 있으니 따라해보시면 됩니다.

### 3.4 실행 결과
소스코드 `test.py`를 실행하시면 `input_text`에 입력된 프롬프트에 대한 답변이 생성됩니다. 비교를 위해 다양한 PC에서 실행해본 결과를 참고해주시기 바랍니다.

<div align="center">

| PC 타입  | CPU            | RAM | 실행 시간(모델 로드 제외) |
|--------|----------------|-----|-----------------|
| 노트북    | Intel i7-1165G7 | 16GB | 2분 30초                |
| 사무용 PC | Intel i5-12400 | 8GB | 13분             |
| 고성능 PC |                |     |                 |

</div>

실행 결과를 보시면 확인하실 수 있듯이, RAM 크기가 작은 경우 실행에 시간이 매우 오래 걸립니다. 노트북에서도 꽤 시간이 소요되는 것을 확인할 수 있는데 소스코드는 32-bit 단정밀도를 사용했기 때문에 16-bit 반정밀도(float16 또는 bfloat16)을 사용하면 실행시간이 더 줄어듭니다. 경우에 따라서는 4bit이나 8bit으로 양자화(quantized)된 모델을 사용할 수 있으므로, 더 경량화된 모델을 구동할 경우 시간은 더 줄일 수 있을 것입니다. 따라서 플래그쉽 스마트폰의 경우에는 거의 실시간으로 작동하는 것을 구현할 수도 있습니다.  

### 4. 마치며
`gemma2` 2B 모델의 성능은 ChatGPT의 초기 버전에 활용된 `GPT-3.5`보다 [우수](https://developers.googleblog.com/en/smaller-safer-more-transparent-advancing-responsible-ai-with-gemma/)합니다.

<div align="center">
<img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma_BlogGraphs_01_20240729_v5.original.png"  title="gemma2-2b-outperform-gpt3.5"></img>
</div>

이 사실은 이제 고성능의 언어 모델이 손에 잡힐 수 있을 정도로 소형화되었다는 것이고, 저사양 PC에서도 실행됨을 확인할 수 있듯이 플래그쉽 스마트폰에서 원활이 구동될 수 있다는 것을 말합니다. 이제 누구나 고성능 언어 모델을 직접 사용할 수 있는 시대가 도래했으니, 강력한 오픈소스 모델로 다양한 인공지능을 개발해보시길 기대합니다.
